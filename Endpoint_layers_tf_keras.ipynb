{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Endpoint layers tf.keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sts-sadr/-Data-Science-A-Z-Real-Life-Data-Science-Exercises-Included-with-DSS/blob/master/Endpoint_layers_tf_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Iu3WjUWC6vI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow --upgrade --quiet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsBSCufaC71Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0aK1hHM9SmL",
        "colab_type": "code",
        "outputId": "c1d7f2f9-502c-4c6c-8add-78bda35a39d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0-dev20190912'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCsztpDc9Wr5",
        "colab_type": "text"
      },
      "source": [
        "# Usage of endpoint layers in the Functional API\n",
        "\n",
        "An \"endpoint layer\" has access to the model's targets, and creates arbitrary losses and metrics using `add_loss` and `add_metric`. This enables you to define losses and metrics that don't match the usual signature `fn(y_true, y_pred, sample_weight=None)`.\n",
        "\n",
        "Note that you could have separate metrics for training and eval with this pattern."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2HH1TUkmtCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticEndpoint(keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, name=None):\n",
        "    super(LogisticEndpoint, self).__init__(name=name)\n",
        "    self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "  def call(self, targets, logits, sample_weights=None):      \n",
        "    # Compute the training-time loss value and add it\n",
        "    # to the layer using `self.add_loss()`.\n",
        "    loss = self.loss_fn(targets, logits, sample_weights)\n",
        "    self.add_loss(loss)\n",
        "    \n",
        "    # Log the loss as a metric (we could log arbitrary metrics,\n",
        "    # including different metrics for training and inference.\n",
        "    self.add_metric(loss, name=self.name, aggregation='mean')\n",
        "\n",
        "    # Return the inference-time prediction tensor (for `.predict()`).\n",
        "    return tf.nn.softmax(logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZh-iJWP9Ttp",
        "colab_type": "code",
        "outputId": "b8597f80-6063-40c8-fbc4-d8d2d5b0e04a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "inputs = keras.Input((764,), name='inputs')\n",
        "logits = keras.layers.Dense(1)(inputs)\n",
        "targets = keras.Input((1,), name='targets')\n",
        "sample_weights = keras.Input((1,), name='sample_weights')\n",
        "preds = LogisticEndpoint()(targets, logits, sample_weights)\n",
        "model = keras.Model([inputs, targets, sample_weights], preds)\n",
        "\n",
        "data = {\n",
        "    'inputs': np.random.random((1000, 764)),\n",
        "    'targets': np.random.random((1000, 1)),\n",
        "    'sample_weights': np.random.random((1000, 1))\n",
        "}\n",
        "\n",
        "model.compile(keras.optimizers.Adam(1e-3))\n",
        "model.fit(data, epochs=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:Output logistic_endpoint missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to logistic_endpoint.\n",
            "Train on 1000 samples\n",
            "Epoch 1/2\n",
            "1000/1000 [==============================] - 1s 572us/sample - loss: 0.3578 - logistic_endpoint: 0.3584\n",
            "Epoch 2/2\n",
            "1000/1000 [==============================] - 0s 61us/sample - loss: 0.3566 - logistic_endpoint: 0.3571\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd64e54aeb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHGlOMCnAZEk",
        "colab_type": "text"
      },
      "source": [
        "# Usage of loss endpoint layers in subclassed models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F45uB239gIf",
        "colab_type": "code",
        "outputId": "a0d47fa3-206c-4cc5-c792-525e84ce87fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "class LogReg(keras.Model):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(LogReg, self).__init__()\n",
        "    self.dense = keras.layers.Dense(1)\n",
        "    self.logistic_endpoint = LogisticEndpoint()\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Note that all inputs should be in the first argument\n",
        "    # since we want to be able to call `model.fit(inputs)`.\n",
        "    logits = self.dense(inputs['inputs'])\n",
        "    preds = self.logistic_endpoint(logits,\n",
        "                                   inputs['targets'],\n",
        "                                   inputs['sample_weights'])\n",
        "    return preds\n",
        "  \n",
        "\n",
        "model = LogReg()\n",
        "data = {\n",
        "    'inputs': np.random.random((1000, 764)),\n",
        "    'targets': np.random.random((1000, 1)),\n",
        "    'sample_weights': np.random.random((1000, 1))\n",
        "}\n",
        "\n",
        "model.compile(keras.optimizers.Adam(1e-3))\n",
        "model.fit(data, epochs=2)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Output output_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to output_1.\n",
            "Train on 1000 samples\n",
            "Epoch 1/2\n",
            "1000/1000 [==============================] - 0s 409us/sample - loss: -0.9798 - logistic_endpoint_1: -1.0056\n",
            "Epoch 2/2\n",
            "1000/1000 [==============================] - 0s 63us/sample - loss: -4.0749 - logistic_endpoint_1: -4.1816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd6554a6240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEg_sLR9Le2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae = VariationalAutoEncoder(784, 64, 32)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
        "vae.fit(x_train, x_train, epochs=3, batch_size=64)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}